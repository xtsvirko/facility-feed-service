# Facility Feed Service

## Overview
Facility Feed Service is an **asynchronous Python service** that:
- **Fetches facility data from PostgreSQL**
- **Generates JSON feed files**
- **Compresses them using GZIP and uploads them to AWS S3**
- **Creates metadata about uploaded files**
- **Runs on a schedule using AWS ECS Fargate**

---

## ğŸ“¦ Technologies

- Python 3.11
- Asyncio, asyncpg
- Docker
- AWS ECS Fargate
- AWS S3
- AWS EventBridge (CloudWatch Events)
- GitHub Actions

## **Project Architecture**


```
facility_feed_service/
â”‚
â”œâ”€â”€ src/                        # ğŸ“¦ Main source code
â”‚   â”œâ”€â”€ database/               # ğŸ“Š Database access layer (PostgreSQL)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py       # Handles async DB connection via asyncpg
â”‚   â”‚   â”œâ”€â”€ queries.py          # Raw SQL queries
â”‚   â”‚   â”œâ”€â”€ repository.py       # Data fetching with pagination
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # âš™ï¸ Core business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feed_generator.py   # Generates JSON feed data
â”‚   â”‚   â”œâ”€â”€ metadata_generator.py # Creates metadata JSON file
â”‚   â”‚   â”œâ”€â”€ s3_uploader.py      # Uploads files to AWS S3 via aioboto3
â”‚   â”‚   â”œâ”€â”€ main.py             # Entry point script (runs on schedule in AWS Fargate)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                  # ğŸ”§ Utility and helper modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py           # Loads environment variables via dotenv
â”‚       â”œâ”€â”€ logger.py           # Logging configuration
â”‚       â”œâ”€â”€ error_handler.py    # Optional error handling helpers
â”‚
â”œâ”€â”€ deploy/                    # ğŸš€ Deployment-related scripts and files
â”‚   â”œâ”€â”€ setup_cloudwatch_event.sh # Bash script to configure AWS CloudWatch EventBridge
â”‚
â”œâ”€â”€ tests/                     # âœ… Unit and integration tests
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_feed_generator.py
â”‚   â”œâ”€â”€ test_metadata_generator.py
â”‚   â”œâ”€â”€ test_s3_uploader.py
â”‚   â”œâ”€â”€ test_scheduler.py
â”‚
â”œâ”€â”€ .github/                   # âš™ï¸ GitHub Actions CI/CD workflows
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml          # CI/CD pipeline configuration
â”‚
â”œâ”€â”€ Dockerfile                 # ğŸ³ Docker container definition
â”œâ”€â”€ .dockerignore              # Files/folders to exclude from Docker builds
â”œâ”€â”€ .env.sample                # Sample environment variable configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation
 
```
## ğŸš€ How to Run
1. Install Dependencies
```bash
  pip install -r requirements.txt
```
2. Set Environment Variables(use .env.sample)
#### Important: 
Make sure you have created database with valid credentials
3. Run the Scheduled Task Manually
    ```bash
    python src/services/main.py
    ```
#### ğŸ³ Optional: Run in Docker
```bash
    docker build -t facility-feed .
    docker run --env-file src/.env facility-feed  
```
## â˜ï¸ Deployment via GitHub Actions
#### Add the following secrets:
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_REGION
- ECR_REPOSITORY
#### After pushing in main docker image will be automatically push the image to AWS ECR

## â±ï¸ Automated Execution via CloudWatch
#### The project is configured to automatically trigger the ECS #task in the #cluster every hour using AWS EventBridge.

#### You can configure it using the deployment script:
```bash
  deploy/setup_cloudwatch_event.sh
```
### Important:
#### This script was tested for the region eu-north-1. For other regions it could be different!